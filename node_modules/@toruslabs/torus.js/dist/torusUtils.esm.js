import stringify from 'json-stable-stringify';
import BN from 'bn.js';
import { keccak256 as keccak256$1 } from 'ethereum-cryptography/keccak';
import loglevel from 'loglevel';
import { generatePrivate, decrypt, getPublic, encrypt } from '@toruslabs/eccrypto';
import _defineProperty from '@babel/runtime/helpers/defineProperty';
import _objectSpread from '@babel/runtime/helpers/objectSpread2';
import { post, generateJsonRPCObject, get, setAPIKey, setEmbedHost } from '@toruslabs/http-helpers';
import { LEGACY_NETWORKS_ROUTE_MAP, SIGNER_MAP, METADATA_MAP } from '@toruslabs/constants';
import { ec } from 'elliptic';

const JRPC_METHODS = {
  GET_OR_SET_KEY: "GetPubKeyOrKeyAssign",
  COMMITMENT_REQUEST: "CommitmentRequest",
  IMPORT_SHARE: "ImportShare",
  GET_SHARE_OR_KEY_ASSIGN: "GetShareOrKeyAssign"
};

// this function normalizes the result from nodes before passing the result to threshold check function
// For ex: some fields returns by nodes might be different from each other
// like created_at field might vary and nonce_data might not be returned by all nodes because
// of the metadata implementation in sapphire.
const normalizeKeysResult = result => {
  const finalResult = {
    keys: [],
    is_new_key: result.is_new_key
  };
  if (result && result.keys && result.keys.length > 0) {
    finalResult.keys = result.keys.map(key => {
      return {
        pub_key_X: key.pub_key_X,
        pub_key_Y: key.pub_key_Y,
        address: key.address
      };
    });
  }
  return finalResult;
};
const kCombinations = (s, k) => {
  let set = s;
  if (typeof set === "number") {
    set = Array.from({
      length: set
    }, (_, i) => i);
  }
  if (k > set.length || k <= 0) {
    return [];
  }
  if (k === set.length) {
    return [set];
  }
  if (k === 1) {
    return set.reduce((acc, cur) => [...acc, [cur]], []);
  }
  const combs = [];
  let tailCombs = [];
  for (let i = 0; i <= set.length - k + 1; i += 1) {
    tailCombs = kCombinations(set.slice(i + 1), k - 1);
    for (let j = 0; j < tailCombs.length; j += 1) {
      combs.push([set[i], ...tailCombs[j]]);
    }
  }
  return combs;
};
const thresholdSame = (arr, t) => {
  const hashMap = {};
  for (let i = 0; i < arr.length; i += 1) {
    const str = stringify(arr[i]);
    hashMap[str] = hashMap[str] ? hashMap[str] + 1 : 1;
    if (hashMap[str] === t) {
      return arr[i];
    }
  }
  return undefined;
};
function encParamsBufToHex(encParams) {
  return {
    iv: Buffer.from(encParams.iv).toString("hex"),
    ephemPublicKey: Buffer.from(encParams.ephemPublicKey).toString("hex"),
    ciphertext: Buffer.from(encParams.ciphertext).toString("hex"),
    mac: Buffer.from(encParams.mac).toString("hex"),
    mode: "AES256"
  };
}
function encParamsHexToBuf(eciesData) {
  return {
    ephemPublicKey: Buffer.from(eciesData.ephemPublicKey, "hex"),
    iv: Buffer.from(eciesData.iv, "hex"),
    mac: Buffer.from(eciesData.mac, "hex")
  };
}

class GetOrSetNonceError extends Error {}

const log = loglevel.getLogger("torus.js");
log.disableAll();

function keccak256(a) {
  const hash = Buffer.from(keccak256$1(a)).toString("hex");
  return `0x${hash}`;
}
function stripHexPrefix(str) {
  return str.startsWith("0x") ? str.slice(2) : str;
}
function toChecksumAddress(hexAddress) {
  const address = stripHexPrefix(hexAddress).toLowerCase();
  const buf = Buffer.from(address, "utf8");
  const hash = Buffer.from(keccak256$1(buf)).toString("hex");
  let ret = "0x";
  for (let i = 0; i < address.length; i++) {
    if (parseInt(hash[i], 16) >= 8) {
      ret += address[i].toUpperCase();
    } else {
      ret += address[i];
    }
  }
  return ret;
}
function generateAddressFromPrivKey(ecCurve, privateKey) {
  const key = ecCurve.keyFromPrivate(privateKey.toString("hex", 64), "hex");
  const publicKey = key.getPublic().encode("hex", false).slice(2);
  log.info(publicKey, "public key");
  const evmAddressLower = `0x${keccak256(Buffer.from(publicKey, "hex")).slice(64 - 38)}`;
  return toChecksumAddress(evmAddressLower);
}
function generateAddressFromPubKey(ecCurve, publicKeyX, publicKeyY) {
  const key = ecCurve.keyFromPublic({
    x: publicKeyX.toString("hex", 64),
    y: publicKeyY.toString("hex", 64)
  });
  const publicKey = key.getPublic().encode("hex", false).slice(2);
  log.info(key.getPublic().encode("hex", false), "public key");
  const evmAddressLower = `0x${keccak256(Buffer.from(publicKey, "hex")).slice(64 - 38)}`;
  return toChecksumAddress(evmAddressLower);
}
function getPostboxKeyFrom1OutOf1(ecCurve, privKey, nonce) {
  const privKeyBN = new BN(privKey, 16);
  const nonceBN = new BN(nonce, 16);
  return privKeyBN.sub(nonceBN).umod(ecCurve.curve.n).toString("hex");
}

class Point {
  constructor(x, y, ecCurve) {
    _defineProperty(this, "x", void 0);
    _defineProperty(this, "y", void 0);
    _defineProperty(this, "ecCurve", void 0);
    this.x = new BN(x, "hex");
    this.y = new BN(y, "hex");
    this.ecCurve = ecCurve;
  }
  encode(enc) {
    switch (enc) {
      case "arr":
        return Buffer.concat([Buffer.from("04", "hex"), Buffer.from(this.x.toString("hex"), "hex"), Buffer.from(this.y.toString("hex"), "hex")]);
      case "elliptic-compressed":
        {
          const key = this.ecCurve.keyFromPublic({
            x: this.x.toString("hex", 64),
            y: this.y.toString("hex", 64)
          }, "hex");
          return Buffer.from(key.getPublic(true, "hex"));
        }
      default:
        throw new Error("encoding doesn't exist in Point");
    }
  }
}

class Share {
  constructor(shareIndex, share) {
    _defineProperty(this, "share", void 0);
    _defineProperty(this, "shareIndex", void 0);
    this.share = new BN(share, "hex");
    this.shareIndex = new BN(shareIndex, "hex");
  }
  static fromJSON(value) {
    const {
      share,
      shareIndex
    } = value;
    return new Share(shareIndex, share);
  }
  toJSON() {
    return {
      share: this.share.toString("hex"),
      shareIndex: this.shareIndex.toString("hex")
    };
  }
}

class Polynomial {
  constructor(polynomial, ecCurve) {
    _defineProperty(this, "polynomial", void 0);
    _defineProperty(this, "ecCurve", void 0);
    this.polynomial = polynomial;
    this.ecCurve = ecCurve;
  }
  getThreshold() {
    return this.polynomial.length;
  }
  polyEval(x) {
    const tmpX = new BN(x, "hex");
    let xi = new BN(tmpX);
    let sum = new BN(0);
    sum = sum.add(this.polynomial[0]);
    for (let i = 1; i < this.polynomial.length; i += 1) {
      const tmp = xi.mul(this.polynomial[i]);
      sum = sum.add(tmp);
      sum = sum.umod(this.ecCurve.curve.n);
      xi = xi.mul(new BN(tmpX));
      xi = xi.umod(this.ecCurve.curve.n);
    }
    return sum;
  }
  generateShares(shareIndexes) {
    const newShareIndexes = shareIndexes.map(index => {
      if (typeof index === "number") {
        return new BN(index);
      }
      if (index instanceof BN) {
        return index;
      }
      if (typeof index === "string") {
        return new BN(index, "hex");
      }
      return index;
    });
    const shares = {};
    for (let x = 0; x < newShareIndexes.length; x += 1) {
      shares[newShareIndexes[x].toString("hex", 64)] = new Share(newShareIndexes[x], this.polyEval(newShareIndexes[x]));
    }
    return shares;
  }
}

function generatePrivateExcludingIndexes(shareIndexes) {
  const key = new BN(generatePrivate());
  if (shareIndexes.find(el => el.eq(key))) {
    return generatePrivateExcludingIndexes(shareIndexes);
  }
  return key;
}
const generateEmptyBNArray = length => Array.from({
  length
}, () => new BN(0));
const denominator = (ecCurve, i, innerPoints) => {
  let result = new BN(1);
  const xi = innerPoints[i].x;
  for (let j = innerPoints.length - 1; j >= 0; j -= 1) {
    if (i !== j) {
      let tmp = new BN(xi);
      tmp = tmp.sub(innerPoints[j].x);
      tmp = tmp.umod(ecCurve.curve.n);
      result = result.mul(tmp);
      result = result.umod(ecCurve.curve.n);
    }
  }
  return result;
};
const interpolationPoly = (ecCurve, i, innerPoints) => {
  let coefficients = generateEmptyBNArray(innerPoints.length);
  const d = denominator(ecCurve, i, innerPoints);
  if (d.cmp(new BN(0)) === 0) {
    throw new Error("Denominator for interpolationPoly is 0");
  }
  coefficients[0] = d.invm(ecCurve.curve.n);
  for (let k = 0; k < innerPoints.length; k += 1) {
    const newCoefficients = generateEmptyBNArray(innerPoints.length);
    if (k !== i) {
      let j;
      if (k < i) {
        j = k + 1;
      } else {
        j = k;
      }
      j -= 1;
      for (; j >= 0; j -= 1) {
        newCoefficients[j + 1] = newCoefficients[j + 1].add(coefficients[j]).umod(ecCurve.curve.n);
        let tmp = new BN(innerPoints[k].x);
        tmp = tmp.mul(coefficients[j]).umod(ecCurve.curve.n);
        newCoefficients[j] = newCoefficients[j].sub(tmp).umod(ecCurve.curve.n);
      }
      coefficients = newCoefficients;
    }
  }
  return coefficients;
};
const pointSort = innerPoints => {
  const pointArrClone = [...innerPoints];
  pointArrClone.sort((a, b) => a.x.cmp(b.x));
  return pointArrClone;
};
const lagrange = (ecCurve, unsortedPoints) => {
  const sortedPoints = pointSort(unsortedPoints);
  const polynomial = generateEmptyBNArray(sortedPoints.length);
  for (let i = 0; i < sortedPoints.length; i += 1) {
    const coefficients = interpolationPoly(ecCurve, i, sortedPoints);
    for (let k = 0; k < sortedPoints.length; k += 1) {
      let tmp = new BN(sortedPoints[i].y);
      tmp = tmp.mul(coefficients[k]);
      polynomial[k] = polynomial[k].add(tmp).umod(ecCurve.curve.n);
    }
  }
  return new Polynomial(polynomial, ecCurve);
};
function lagrangeInterpolatePolynomial(ecCurve, points) {
  return lagrange(ecCurve, points);
}
function lagrangeInterpolation(ecCurve, shares, nodeIndex) {
  if (shares.length !== nodeIndex.length) {
    throw new Error("shares not equal to nodeIndex length in lagrangeInterpolation");
  }
  let secret = new BN(0);
  for (let i = 0; i < shares.length; i += 1) {
    let upper = new BN(1);
    let lower = new BN(1);
    for (let j = 0; j < shares.length; j += 1) {
      if (i !== j) {
        upper = upper.mul(nodeIndex[j].neg());
        upper = upper.umod(ecCurve.curve.n);
        let temp = nodeIndex[i].sub(nodeIndex[j]);
        temp = temp.umod(ecCurve.curve.n);
        lower = lower.mul(temp).umod(ecCurve.curve.n);
      }
    }
    let delta = upper.mul(lower.invm(ecCurve.curve.n)).umod(ecCurve.curve.n);
    delta = delta.mul(shares[i]).umod(ecCurve.curve.n);
    secret = secret.add(delta);
  }
  return secret.umod(ecCurve.curve.n);
}

// generateRandomPolynomial - determinisiticShares are assumed random
function generateRandomPolynomial(ecCurve, degree, secret, deterministicShares) {
  let actualS = secret;
  if (!secret) {
    actualS = generatePrivateExcludingIndexes([new BN(0)]);
  }
  if (!deterministicShares) {
    const poly = [actualS];
    for (let i = 0; i < degree; i += 1) {
      const share = generatePrivateExcludingIndexes(poly);
      poly.push(share);
    }
    return new Polynomial(poly, ecCurve);
  }
  if (!Array.isArray(deterministicShares)) {
    throw new Error("deterministic shares in generateRandomPolynomial should be an array");
  }
  if (deterministicShares.length > degree) {
    throw new Error("deterministicShares in generateRandomPolynomial should be less or equal than degree to ensure an element of randomness");
  }
  const points = {};
  deterministicShares.forEach(share => {
    points[share.shareIndex.toString("hex", 64)] = new Point(share.shareIndex, share.share, ecCurve);
  });
  for (let i = 0; i < degree - deterministicShares.length; i += 1) {
    let shareIndex = generatePrivateExcludingIndexes([new BN(0)]);
    while (points[shareIndex.toString("hex", 64)] !== undefined) {
      shareIndex = generatePrivateExcludingIndexes([new BN(0)]);
    }
    points[shareIndex.toString("hex", 64)] = new Point(shareIndex, new BN(generatePrivate()), ecCurve);
  }
  points["0"] = new Point(new BN(0), actualS, ecCurve);
  return lagrangeInterpolatePolynomial(ecCurve, Object.values(points));
}

function convertMetadataToNonce(params) {
  if (!params || !params.message) {
    return new BN(0);
  }
  return new BN(params.message, 16);
}
async function decryptNodeData(eciesData, ciphertextHex, privKey) {
  const metadata = encParamsHexToBuf(eciesData);
  const decryptedSigBuffer = await decrypt(privKey, _objectSpread(_objectSpread({}, metadata), {}, {
    ciphertext: Buffer.from(ciphertextHex, "hex")
  }));
  return decryptedSigBuffer;
}
function generateMetadataParams(ecCurve, serverTimeOffset, message, privateKey) {
  const key = ecCurve.keyFromPrivate(privateKey.toString("hex", 64));
  const setData = {
    data: message,
    timestamp: new BN(~~(serverTimeOffset + Date.now() / 1000)).toString(16)
  };
  const sig = key.sign(keccak256(Buffer.from(stringify(setData), "utf8")).slice(2));
  return {
    pub_key_X: key.getPublic().getX().toString("hex"),
    // DO NOT PAD THIS. BACKEND DOESN'T
    pub_key_Y: key.getPublic().getY().toString("hex"),
    // DO NOT PAD THIS. BACKEND DOESN'T
    set_data: setData,
    signature: Buffer.from(sig.r.toString(16, 64) + sig.s.toString(16, 64) + new BN("").toString(16, 2), "hex").toString("base64")
  };
}
async function getMetadata(legacyMetadataHost, data) {
  let options = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : {};
  try {
    const metadataResponse = await post(`${legacyMetadataHost}/get`, data, options, {
      useAPIKey: true
    });
    if (!metadataResponse || !metadataResponse.message) {
      return new BN(0);
    }
    return new BN(metadataResponse.message, 16); // nonce
  } catch (error) {
    loglevel.error("get metadata error", error);
    return new BN(0);
  }
}
async function getOrSetNonce(legacyMetadataHost, ecCurve, serverTimeOffset, X, Y, privKey) {
  let getOnly = arguments.length > 6 && arguments[6] !== undefined ? arguments[6] : false;
  let data;
  const msg = getOnly ? "getNonce" : "getOrSetNonce";
  if (privKey) {
    data = generateMetadataParams(ecCurve, serverTimeOffset, msg, privKey);
  } else {
    data = {
      pub_key_X: X,
      pub_key_Y: Y,
      set_data: {
        data: msg
      }
    };
  }
  return post(`${legacyMetadataHost}/get_or_set_nonce`, data, undefined, {
    useAPIKey: true
  });
}
async function getNonce(legacyMetadataHost, ecCurve, serverTimeOffset, X, Y, privKey) {
  return getOrSetNonce(legacyMetadataHost, ecCurve, serverTimeOffset, X, Y, privKey, true);
}

const config = {
  logRequestTracing: false
};

function capitalizeFirstLetter(str) {
  return str.charAt(0).toUpperCase() + str.slice(1);
}
class SomeError extends Error {
  constructor(_ref) {
    let {
      errors,
      responses,
      predicate
    } = _ref;
    super("Unable to resolve enough promises.");
    _defineProperty(this, "errors", void 0);
    _defineProperty(this, "responses", void 0);
    _defineProperty(this, "predicate", void 0);
    this.errors = errors;
    this.responses = responses;
    this.predicate = predicate;
  }
  get message() {
    return `${super.message}. ${this.errors.length} errors: ${this.errors.map(x => x.message || x).join(", ")} and ${this.responses.length} responses: ${JSON.stringify(this.responses)}`;
  }
  toString() {
    return this.message;
  }
}
const Some = (promises, predicate) => new Promise((resolve, reject) => {
  let finishedCount = 0;
  const sharedState = {
    resolved: false
  };
  const errorArr = new Array(promises.length).fill(undefined);
  const resultArr = new Array(promises.length).fill(undefined);
  let predicateError;
  // eslint-disable-next-line no-promise-executor-return
  return promises.forEach((x, index) => {
    return x.then(resp => {
      resultArr[index] = resp;
      return undefined;
    }).catch(error => {
      errorArr[index] = error;
    })
    // eslint-disable-next-line promise/no-return-in-finally
    .finally(() => {
      if (sharedState.resolved) return;
      return predicate(resultArr.slice(0), sharedState).then(data => {
        sharedState.resolved = true;
        resolve(data);
        return undefined;
      }).catch(error => {
        // log only the last predicate error
        predicateError = error;
      }).finally(() => {
        finishedCount += 1;
        if (finishedCount === promises.length) {
          const errors = Object.values(resultArr.reduce((acc, z) => {
            if (z) {
              var _error$data;
              const {
                id,
                error
              } = z;
              if ((error === null || error === void 0 || (_error$data = error.data) === null || _error$data === void 0 ? void 0 : _error$data.length) > 0) {
                if (error.data.startsWith("Error occurred while verifying params")) acc[id] = capitalizeFirstLetter(error.data);else acc[id] = error.data;
              }
            }
            return acc;
          }, {}));
          if (errors.length > 0) {
            // Format-able errors
            const msg = errors.length > 1 ? `\n${errors.map(it => `• ${it}`).join("\n")}` : errors[0];
            reject(new Error(msg));
          } else {
            var _predicateError;
            reject(new SomeError({
              errors: errorArr,
              responses: resultArr,
              predicate: ((_predicateError = predicateError) === null || _predicateError === void 0 ? void 0 : _predicateError.message) || predicateError
            }));
          }
        }
      });
    });
  });
});

const GetPubKeyOrKeyAssign = async params => {
  const {
    endpoints,
    network,
    verifier,
    verifierId,
    extendedVerifierId
  } = params;
  const lookupPromises = endpoints.map(x => post(x, generateJsonRPCObject(JRPC_METHODS.GET_OR_SET_KEY, {
    verifier,
    verifier_id: verifierId.toString(),
    extended_verifier_id: extendedVerifierId,
    one_key_flow: true,
    fetch_node_index: true
  }), null, {
    logTracingHeader: config.logRequestTracing
  }).catch(err => log.error(`${JRPC_METHODS.GET_OR_SET_KEY} request failed`, err)));
  let nonceResult;
  const nodeIndexes = [];
  const result = await Some(lookupPromises, lookupResults => {
    const lookupPubKeys = lookupResults.filter(x1 => {
      if (x1 && !x1.error) {
        if (!nonceResult) {
          var _x1$result;
          // currently only one node returns metadata nonce
          // other nodes returns empty object
          // pubNonce must be available to derive the public key
          const pubNonceX = (_x1$result = x1.result) === null || _x1$result === void 0 || (_x1$result = _x1$result.keys[0].nonce_data) === null || _x1$result === void 0 || (_x1$result = _x1$result.pubNonce) === null || _x1$result === void 0 ? void 0 : _x1$result.x;
          if (pubNonceX) {
            nonceResult = x1.result.keys[0].nonce_data;
          }
        }
        return x1;
      }
      return false;
    });
    const errorResult = thresholdSame(lookupPubKeys.map(x2 => x2 && x2.error), ~~(endpoints.length / 2) + 1);
    const keyResult = thresholdSame(lookupPubKeys.map(x3 => x3 && normalizeKeysResult(x3.result)), ~~(endpoints.length / 2) + 1);

    // nonceResult must exist except for extendedVerifierId and legacy networks along with keyResult
    if (keyResult && (nonceResult || extendedVerifierId || LEGACY_NETWORKS_ROUTE_MAP[network]) || errorResult) {
      if (keyResult) {
        lookupResults.forEach(x1 => {
          if (x1 && x1.result) {
            const currentNodePubKey = x1.result.keys[0].pub_key_X.toLowerCase();
            const thresholdPubKey = keyResult.keys[0].pub_key_X.toLowerCase();
            // push only those indexes for nodes who are returning pub key matching with threshold pub key.
            // this check is important when different nodes have different keys assigned to a user.
            if (currentNodePubKey === thresholdPubKey) {
              const nodeIndex = parseInt(x1.result.node_index);
              if (nodeIndex) nodeIndexes.push(nodeIndex);
            }
          }
        });
      }
      return Promise.resolve({
        keyResult,
        nodeIndexes,
        errorResult,
        nonceResult
      });
    }
    return Promise.reject(new Error(`invalid public key result: ${JSON.stringify(lookupResults)} and nonce result:${JSON.stringify(nonceResult || {})} for verifier: ${verifier}, verifierId: ${verifierId} and extendedVerifierId: ${extendedVerifierId} `));
  });
  return result;
};
async function retrieveOrImportShare(params) {
  const {
    legacyMetadataHost,
    serverTimeOffset,
    enableOneKey,
    ecCurve,
    allowHost,
    network,
    clientId,
    endpoints,
    verifier,
    verifierParams,
    idToken,
    importedShares,
    extraParams
  } = params;
  await get(allowHost, {
    headers: {
      verifier,
      verifierId: verifierParams.verifier_id,
      network,
      clientId,
      enableGating: "true"
    }
  }, {
    useAPIKey: true
  });
  const promiseArr = [];

  // generate temporary private and public key that is used to secure receive shares
  const sessionAuthKey = generatePrivate();
  const pubKey = getPublic(sessionAuthKey).toString("hex");
  const pubKeyX = pubKey.slice(2, 66);
  const pubKeyY = pubKey.slice(66);
  const tokenCommitment = keccak256(Buffer.from(idToken, "utf8"));
  let isImportShareReq = false;
  if (importedShares && importedShares.length > 0) {
    if (importedShares.length !== endpoints.length) {
      throw new Error("Invalid imported shares length");
    }
    isImportShareReq = true;
  }

  // make commitment requests to endpoints
  for (let i = 0; i < endpoints.length; i += 1) {
    /*
      CommitmentRequestParams struct {
        MessagePrefix      string `json:"messageprefix"`
        TokenCommitment    string `json:"tokencommitment"`
        TempPubX           string `json:"temppubx"`
        TempPubY           string `json:"temppuby"`
        VerifierIdentifier string `json:"verifieridentifier"`
      } 
      */
    const p = post(endpoints[i], generateJsonRPCObject(JRPC_METHODS.COMMITMENT_REQUEST, {
      messageprefix: "mug00",
      tokencommitment: tokenCommitment.slice(2),
      temppubx: pubKeyX,
      temppuby: pubKeyY,
      verifieridentifier: verifier
    }), null, {
      logTracingHeader: config.logRequestTracing
    }).catch(err => {
      log.error("commitment error", err);
    });
    promiseArr.push(p);
  }
  // send share request once k + t number of commitment requests have completed
  return Some(promiseArr, resultArr => {
    const completedRequests = resultArr.filter(x => {
      if (!x || typeof x !== "object") {
        return false;
      }
      if (x.error) {
        return false;
      }
      return true;
    });

    // we need to get commitments from all endpoints for importing share
    if (importedShares.length > 0 && completedRequests.length === endpoints.length) {
      return Promise.resolve(resultArr);
    } else if (importedShares.length === 0 && completedRequests.length >= ~~(endpoints.length * 3 / 4) + 1) {
      const requiredNodeResult = completedRequests.find(resp => {
        var _resp$result;
        if (resp && ((_resp$result = resp.result) === null || _resp$result === void 0 ? void 0 : _resp$result.nodeindex) === "1") {
          return true;
        }
        return false;
      });
      if (requiredNodeResult) {
        return Promise.resolve(resultArr);
      }
    }
    return Promise.reject(new Error(`invalid ${JSON.stringify(resultArr)}`));
  }).then(responses => {
    const promiseArrRequest = [];
    const nodeSigs = [];
    for (let i = 0; i < responses.length; i += 1) {
      const x = responses[i];
      if (!x || typeof x !== "object") {
        continue;
      }
      if (x.error) {
        continue;
      }
      if (x) nodeSigs.push(x.result);
    }
    for (let i = 0; i < endpoints.length; i += 1) {
      const x = responses[i];
      if (!x || typeof x !== "object") {
        continue;
      }
      if (x.error) {
        continue;
      }
      if (isImportShareReq) {
        const importedShare = importedShares[i];
        const p = post(endpoints[i], generateJsonRPCObject(JRPC_METHODS.IMPORT_SHARE, {
          encrypted: "yes",
          use_temp: true,
          item: [_objectSpread(_objectSpread({}, verifierParams), {}, {
            idtoken: idToken,
            nodesignatures: nodeSigs,
            verifieridentifier: verifier,
            pub_key_x: importedShare.pub_key_x,
            pub_key_y: importedShare.pub_key_y,
            encrypted_share: importedShare.encrypted_share,
            encrypted_share_metadata: importedShare.encrypted_share_metadata,
            node_index: importedShare.node_index,
            key_type: importedShare.key_type,
            nonce_data: importedShare.nonce_data,
            nonce_signature: importedShare.nonce_signature
          }, extraParams)],
          one_key_flow: true
        }), null, {
          logTracingHeader: config.logRequestTracing
        }).catch(err => log.error("share req", err));
        promiseArrRequest.push(p);
      } else {
        const p = post(endpoints[i], generateJsonRPCObject(JRPC_METHODS.GET_SHARE_OR_KEY_ASSIGN, {
          encrypted: "yes",
          use_temp: true,
          item: [_objectSpread(_objectSpread({}, verifierParams), {}, {
            idtoken: idToken,
            nodesignatures: nodeSigs,
            verifieridentifier: verifier
          }, extraParams)],
          one_key_flow: true
        }), null, {
          logTracingHeader: config.logRequestTracing
        }).catch(err => log.error("share req", err));
        promiseArrRequest.push(p);
      }
    }
    let thresholdNonceData;
    return Some(promiseArrRequest, async (shareResponses, sharedState) => {
      // check if threshold number of nodes have returned the same user public key
      const completedRequests = shareResponses.filter(x => {
        if (!x || typeof x !== "object") {
          return false;
        }
        if (x.error) {
          return false;
        }
        return true;
      });
      const pubkeys = shareResponses.map(x => {
        if (x && x.result && x.result.keys[0].public_key) {
          if (!thresholdNonceData && !verifierParams.extended_verifier_id) {
            var _x$result$keys$0$nonc;
            const pubNonce = (_x$result$keys$0$nonc = x.result.keys[0].nonce_data) === null || _x$result$keys$0$nonc === void 0 || (_x$result$keys$0$nonc = _x$result$keys$0$nonc.pubNonce) === null || _x$result$keys$0$nonc === void 0 ? void 0 : _x$result$keys$0$nonc.x;
            if (pubNonce) {
              thresholdNonceData = x.result.keys[0].nonce_data;
            }
          }
          return x.result.keys[0].public_key;
        }
        return undefined;
      });
      const thresholdPublicKey = thresholdSame(pubkeys, ~~(endpoints.length / 2) + 1);
      if (!thresholdPublicKey) {
        throw new Error("invalid result from nodes, threshold number of public key results are not matching");
      }

      // if both thresholdNonceData and extended_verifier_id are not available
      // then we need to throw other wise address would be incorrect.
      if (!thresholdNonceData && !verifierParams.extended_verifier_id && !LEGACY_NETWORKS_ROUTE_MAP[network]) {
        throw new Error(`invalid metadata result from nodes, nonce metadata is empty for verifier: ${verifier} and verifierId: ${verifierParams.verifier_id}`);
      }
      const thresholdReqCount = importedShares.length > 0 ? endpoints.length : ~~(endpoints.length / 2) + 1;
      // optimistically run lagrange interpolation once threshold number of shares have been received
      // this is matched against the user public key to ensure that shares are consistent
      // Note: no need of thresholdMetadataNonce for extended_verifier_id key
      if (completedRequests.length >= thresholdReqCount && thresholdPublicKey && (thresholdNonceData || verifierParams.extended_verifier_id || LEGACY_NETWORKS_ROUTE_MAP[network])) {
        const sharePromises = [];
        const sessionTokenSigPromises = [];
        const sessionTokenPromises = [];
        const nodeIndexes = [];
        const sessionTokenData = [];
        const isNewKeyResponses = [];
        for (let i = 0; i < completedRequests.length; i += 1) {
          const currentShareResponse = completedRequests[i];
          const {
            session_tokens: sessionTokens,
            session_token_metadata: sessionTokenMetadata,
            session_token_sigs: sessionTokenSigs,
            session_token_sig_metadata: sessionTokenSigMetadata,
            keys,
            is_new_key: isNewKey
          } = currentShareResponse.result;
          isNewKeyResponses.push(isNewKey);
          if ((sessionTokenSigs === null || sessionTokenSigs === void 0 ? void 0 : sessionTokenSigs.length) > 0) {
            var _sessionTokenSigMetad;
            // decrypt sessionSig if enc metadata is sent
            if (sessionTokenSigMetadata && (_sessionTokenSigMetad = sessionTokenSigMetadata[0]) !== null && _sessionTokenSigMetad !== void 0 && _sessionTokenSigMetad.ephemPublicKey) {
              sessionTokenSigPromises.push(decryptNodeData(sessionTokenSigMetadata[0], sessionTokenSigs[0], sessionAuthKey).catch(err => log.debug("session sig decryption", err)));
            } else {
              sessionTokenSigPromises.push(Promise.resolve(Buffer.from(sessionTokenSigs[0], "hex")));
            }
          } else {
            sessionTokenSigPromises.push(Promise.resolve(undefined));
          }
          if ((sessionTokens === null || sessionTokens === void 0 ? void 0 : sessionTokens.length) > 0) {
            var _sessionTokenMetadata;
            // decrypt session token if enc metadata is sent
            if (sessionTokenMetadata && (_sessionTokenMetadata = sessionTokenMetadata[0]) !== null && _sessionTokenMetadata !== void 0 && _sessionTokenMetadata.ephemPublicKey) {
              sessionTokenPromises.push(decryptNodeData(sessionTokenMetadata[0], sessionTokens[0], sessionAuthKey).catch(err => log.debug("session token sig decryption", err)));
            } else {
              sessionTokenPromises.push(Promise.resolve(Buffer.from(sessionTokens[0], "base64")));
            }
          } else {
            sessionTokenPromises.push(Promise.resolve(undefined));
          }
          if ((keys === null || keys === void 0 ? void 0 : keys.length) > 0) {
            const latestKey = currentShareResponse.result.keys[0];
            nodeIndexes.push(new BN(latestKey.node_index));
            if (latestKey.share_metadata) {
              sharePromises.push(decryptNodeData(latestKey.share_metadata, Buffer.from(latestKey.share, "base64").toString("binary").padStart(64, "0"), sessionAuthKey).catch(err => log.debug("share decryption", err)));
            }
          } else {
            nodeIndexes.push(undefined);
            sharePromises.push(Promise.resolve(undefined));
          }
        }
        const allPromises = await Promise.all(sharePromises.concat(sessionTokenSigPromises).concat(sessionTokenPromises));
        const sharesResolved = allPromises.slice(0, sharePromises.length);
        const sessionSigsResolved = allPromises.slice(sharePromises.length, sharePromises.length + sessionTokenSigPromises.length);
        const sessionTokensResolved = allPromises.slice(sharePromises.length + sessionTokenSigPromises.length, allPromises.length);
        const validSigs = sessionSigsResolved.filter(sig => {
          if (sig) {
            return true;
          }
          return false;
        });
        const minThresholdRequired = ~~(endpoints.length / 2) + 1;
        if (!verifierParams.extended_verifier_id && validSigs.length < minThresholdRequired) {
          throw new Error(`Insufficient number of signatures from nodes, required: ${minThresholdRequired}, found: ${validSigs.length}`);
        }
        const validTokens = sessionTokensResolved.filter(token => {
          if (token) {
            return true;
          }
          return false;
        });
        if (!verifierParams.extended_verifier_id && validTokens.length < minThresholdRequired) {
          throw new Error(`Insufficient number of session tokens from nodes, required: ${minThresholdRequired}, found: ${validTokens.length}`);
        }
        sessionTokensResolved.forEach((x, index) => {
          if (!x) sessionTokenData.push(undefined);else sessionTokenData.push({
            token: x.toString("base64"),
            signature: sessionSigsResolved[index].toString("hex"),
            node_pubx: completedRequests[index].result.node_pubx,
            node_puby: completedRequests[index].result.node_puby
          });
        });
        if (sharedState.resolved) return undefined;
        const decryptedShares = sharesResolved.reduce((acc, curr, index) => {
          if (curr) acc.push({
            index: nodeIndexes[index],
            value: new BN(curr)
          });
          return acc;
        }, []);
        // run lagrange interpolation on all subsets, faster in the optimistic scenario than berlekamp-welch due to early exit
        const allCombis = kCombinations(decryptedShares.length, ~~(endpoints.length / 2) + 1);
        let privateKey = null;
        for (let j = 0; j < allCombis.length; j += 1) {
          const currentCombi = allCombis[j];
          const currentCombiShares = decryptedShares.filter((_, index) => currentCombi.includes(index));
          const shares = currentCombiShares.map(x => x.value);
          const indices = currentCombiShares.map(x => x.index);
          const derivedPrivateKey = lagrangeInterpolation(ecCurve, shares, indices);
          if (!derivedPrivateKey) continue;
          const decryptedPubKey = getPublic(Buffer.from(derivedPrivateKey.toString(16, 64), "hex")).toString("hex");
          const decryptedPubKeyX = decryptedPubKey.slice(2, 66);
          const decryptedPubKeyY = decryptedPubKey.slice(66);
          if (new BN(decryptedPubKeyX, 16).cmp(new BN(thresholdPublicKey.X, 16)) === 0 && new BN(decryptedPubKeyY, 16).cmp(new BN(thresholdPublicKey.Y, 16)) === 0) {
            privateKey = derivedPrivateKey;
            break;
          }
        }
        if (privateKey === undefined || privateKey === null) {
          throw new Error("could not derive private key");
        }
        const thresholdIsNewKey = thresholdSame(isNewKeyResponses, ~~(endpoints.length / 2) + 1);
        return {
          privateKey,
          sessionTokenData,
          thresholdNonceData,
          nodeIndexes,
          isNewKey: thresholdIsNewKey === "true"
        };
      }
      throw new Error("Invalid");
    });
  }).then(async res => {
    var _nonceResult;
    const {
      privateKey,
      sessionTokenData,
      thresholdNonceData,
      nodeIndexes,
      isNewKey
    } = res;
    let nonceResult = thresholdNonceData;
    if (!privateKey) throw new Error("Invalid private key returned");
    const oAuthKey = privateKey;
    const oAuthPubKey = getPublic(Buffer.from(oAuthKey.toString(16, 64), "hex")).toString("hex");
    const oAuthPubkeyX = oAuthPubKey.slice(2, 66);
    const oAuthPubkeyY = oAuthPubKey.slice(66);
    let metadataNonce = new BN((_nonceResult = nonceResult) !== null && _nonceResult !== void 0 && _nonceResult.nonce ? nonceResult.nonce.padStart(64, "0") : "0", "hex");
    let finalPubKey;
    let pubNonce;
    let typeOfUser = "v1";
    // extended_verifier_id is only exception for torus-test-health verifier
    // otherwise extended verifier id should not even return shares.
    if (verifierParams.extended_verifier_id) {
      typeOfUser = "v2";
      // for tss key no need to add pub nonce
      finalPubKey = ecCurve.keyFromPublic({
        x: oAuthPubkeyX,
        y: oAuthPubkeyY
      }).getPublic();
    } else if (LEGACY_NETWORKS_ROUTE_MAP[network]) {
      if (enableOneKey) {
        nonceResult = await getOrSetNonce(legacyMetadataHost, ecCurve, serverTimeOffset, oAuthPubkeyX, oAuthPubkeyY, oAuthKey, !isNewKey);
        metadataNonce = new BN(nonceResult.nonce || "0", 16);
        typeOfUser = nonceResult.typeOfUser;
        if (typeOfUser === "v2") {
          pubNonce = {
            X: nonceResult.pubNonce.x,
            Y: nonceResult.pubNonce.y
          };
          finalPubKey = ecCurve.keyFromPublic({
            x: oAuthPubkeyX,
            y: oAuthPubkeyY
          }).getPublic().add(ecCurve.keyFromPublic({
            x: nonceResult.pubNonce.x,
            y: nonceResult.pubNonce.y
          }).getPublic());
        } else {
          typeOfUser = "v1";
          // for imported keys in legacy networks
          metadataNonce = await getMetadata(legacyMetadataHost, {
            pub_key_X: oAuthPubkeyX,
            pub_key_Y: oAuthPubkeyY
          });
          const privateKeyWithNonce = oAuthKey.add(metadataNonce).umod(ecCurve.curve.n);
          finalPubKey = ecCurve.keyFromPrivate(privateKeyWithNonce.toString(16, 64), "hex").getPublic();
        }
      } else {
        typeOfUser = "v1";
        // for imported keys in legacy networks
        metadataNonce = await getMetadata(legacyMetadataHost, {
          pub_key_X: oAuthPubkeyX,
          pub_key_Y: oAuthPubkeyY
        });
        const privateKeyWithNonce = oAuthKey.add(metadataNonce).umod(ecCurve.curve.n);
        finalPubKey = ecCurve.keyFromPrivate(privateKeyWithNonce.toString(16, 64), "hex").getPublic();
      }
    } else {
      typeOfUser = "v2";
      finalPubKey = ecCurve.keyFromPublic({
        x: oAuthPubkeyX,
        y: oAuthPubkeyY
      }).getPublic().add(ecCurve.keyFromPublic({
        x: nonceResult.pubNonce.x,
        y: nonceResult.pubNonce.y
      }).getPublic());
      pubNonce = {
        X: nonceResult.pubNonce.x,
        Y: nonceResult.pubNonce.y
      };
    }
    if (!finalPubKey) {
      throw new Error("Invalid public key, this might be a bug, please report this to web3auth team");
    }
    const oAuthKeyAddress = generateAddressFromPrivKey(ecCurve, oAuthKey);

    // deriving address from pub key coz pubkey is always available
    // but finalPrivKey won't be available for  v2 user upgraded to 2/n
    const finalEvmAddress = generateAddressFromPubKey(ecCurve, finalPubKey.getX(), finalPubKey.getY());
    log.debug("> torus.js/retrieveShares", {
      finalEvmAddress
    });
    let finalPrivKey = ""; // it is empty for v2 user upgraded to 2/n
    if (typeOfUser === "v1" || typeOfUser === "v2" && metadataNonce.gt(new BN(0))) {
      const privateKeyWithNonce = oAuthKey.add(metadataNonce).umod(ecCurve.curve.n);
      finalPrivKey = privateKeyWithNonce.toString("hex", 64);
    }
    let isUpgraded = false;
    if (typeOfUser === "v1") {
      isUpgraded = null;
    } else if (typeOfUser === "v2") {
      isUpgraded = metadataNonce.eq(new BN("0"));
    }
    // return reconstructed private key and ethereum address
    return {
      finalKeyData: {
        evmAddress: finalEvmAddress,
        X: finalPubKey.getX().toString(16, 64),
        // this is final pub x user before and after updating to 2/n
        Y: finalPubKey.getY().toString(16, 64),
        // this is final pub y user before and after updating to 2/n
        privKey: finalPrivKey
      },
      oAuthKeyData: {
        evmAddress: oAuthKeyAddress,
        X: oAuthPubkeyX,
        Y: oAuthPubkeyY,
        privKey: oAuthKey.toString("hex", 64).padStart(64, "0")
      },
      sessionData: {
        sessionTokenData,
        sessionAuthKey: sessionAuthKey.toString("hex").padStart(64, "0")
      },
      metadata: {
        pubNonce,
        nonce: metadataNonce,
        typeOfUser,
        upgraded: isUpgraded
      },
      nodesData: {
        nodeIndexes: nodeIndexes.map(x => x.toNumber())
      }
    };
  });
}
const legacyKeyLookup = async (endpoints, verifier, verifierId) => {
  const lookupPromises = endpoints.map(x => post(x, generateJsonRPCObject("VerifierLookupRequest", {
    verifier,
    verifier_id: verifierId.toString()
  })).catch(err => log.error("lookup request failed", err)));
  return Some(lookupPromises, lookupResults => {
    const lookupShares = lookupResults.filter(x1 => x1);
    const errorResult = thresholdSame(lookupShares.map(x2 => x2 && x2.error), ~~(endpoints.length / 2) + 1);
    const keyResult = thresholdSame(lookupShares.map(x3 => x3 && x3.result), ~~(endpoints.length / 2) + 1);
    if (keyResult || errorResult) {
      return Promise.resolve({
        keyResult,
        errorResult
      });
    }
    return Promise.reject(new Error(`invalid results ${JSON.stringify(lookupResults)}`));
  });
};
const legacyKeyAssign = async _ref => {
  let {
    endpoints,
    torusNodePubs,
    lastPoint,
    firstPoint,
    verifier,
    verifierId,
    signerHost,
    network,
    clientId
  } = _ref;
  let nodeNum;
  let initialPoint;
  if (lastPoint === undefined) {
    nodeNum = Math.floor(Math.random() * endpoints.length);
    // nodeNum = endpoints.indexOf("https://torus-node.binancex.dev/jrpc");
    log.info("keyassign", nodeNum, endpoints[nodeNum]);
    initialPoint = nodeNum;
  } else {
    nodeNum = lastPoint % endpoints.length;
  }
  if (nodeNum === firstPoint) throw new Error("Looped through all");
  if (firstPoint !== undefined) initialPoint = firstPoint;
  const data = generateJsonRPCObject("KeyAssign", {
    verifier,
    verifier_id: verifierId.toString()
  });
  try {
    const signedData = await post(signerHost, data, {
      headers: {
        pubKeyX: torusNodePubs[nodeNum].X,
        pubKeyY: torusNodePubs[nodeNum].Y,
        network,
        clientId
      }
    }, {
      useAPIKey: true
    });
    return await post(endpoints[nodeNum], _objectSpread(_objectSpread({}, data), signedData), {
      headers: {
        "Content-Type": "application/json; charset=utf-8"
      }
    });
  } catch (error2) {
    const error = error2;
    log.error(error.status, error.message, error, "key assign error");
    const acceptedErrorMsgs = [
    // Slow node
    "Timed out", "Failed to fetch", "cancelled", "NetworkError when attempting to fetch resource.",
    // Happens when the node is not reachable (dns issue etc)
    "TypeError: Failed to fetch",
    // All except iOS and Firefox
    "TypeError: cancelled",
    // iOS
    "TypeError: NetworkError when attempting to fetch resource." // Firefox
    ];

    if ((error === null || error === void 0 ? void 0 : error.status) === 502 || (error === null || error === void 0 ? void 0 : error.status) === 504 || (error === null || error === void 0 ? void 0 : error.status) === 401 || acceptedErrorMsgs.includes(error.message) || acceptedErrorMsgs.some(x => {
      var _error$message;
      return (_error$message = error.message) === null || _error$message === void 0 ? void 0 : _error$message.includes(x);
    }) || error.message && error.message.includes("reason: getaddrinfo EAI_AGAIN")) return legacyKeyAssign({
      endpoints,
      torusNodePubs,
      lastPoint: nodeNum + 1,
      firstPoint: initialPoint,
      verifier,
      verifierId,
      signerHost,
      network,
      clientId
    });
    throw new Error(`Sorry, the Torus Network that powers Web3Auth is currently very busy.
    We will generate your key in time. Pls try again later. \n
    ${error.message || ""}`);
  }
};
const legacyWaitKeyLookup = (endpoints, verifier, verifierId, timeout) => new Promise((resolve, reject) => {
  setTimeout(() => {
    legacyKeyLookup(endpoints, verifier, verifierId).then(resolve).catch(reject);
  }, timeout);
});

// Implement threshold logic wrappers around public APIs
// of Torus nodes to handle malicious node responses
class Torus {
  constructor(_ref) {
    let {
      enableOneKey = false,
      clientId,
      network,
      serverTimeOffset = 0,
      allowHost,
      legacyMetadataHost
    } = _ref;
    _defineProperty(this, "allowHost", void 0);
    _defineProperty(this, "serverTimeOffset", void 0);
    _defineProperty(this, "network", void 0);
    _defineProperty(this, "clientId", void 0);
    _defineProperty(this, "ec", void 0);
    _defineProperty(this, "enableOneKey", void 0);
    _defineProperty(this, "signerHost", void 0);
    _defineProperty(this, "legacyMetadataHost", void 0);
    if (!clientId) throw Error("Please provide a valid clientId in constructor");
    if (!network) throw Error("Please provide a valid network in constructor");
    this.ec = new ec("secp256k1");
    this.serverTimeOffset = serverTimeOffset || 0; // ms
    this.network = network;
    this.clientId = clientId;
    this.allowHost = allowHost || `${SIGNER_MAP[network]}/api/allow`;
    this.enableOneKey = enableOneKey;
    this.legacyMetadataHost = legacyMetadataHost || METADATA_MAP[network];
    this.signerHost = `${SIGNER_MAP[network]}/api/sign`;
  }
  get isLegacyNetwork() {
    const legacyNetwork = LEGACY_NETWORKS_ROUTE_MAP[this.network];
    if (legacyNetwork && !legacyNetwork.migrationCompleted) return true;
    return false;
  }
  static enableLogging() {
    let v = arguments.length > 0 && arguments[0] !== undefined ? arguments[0] : true;
    if (v) {
      log.enableAll();
      config.logRequestTracing = true;
    } else log.disableAll();
  }
  static setAPIKey(apiKey) {
    setAPIKey(apiKey);
  }
  static setEmbedHost(embedHost) {
    setEmbedHost(embedHost);
  }
  static isGetOrSetNonceError(err) {
    return err instanceof GetOrSetNonceError;
  }
  static getPostboxKey(torusKey) {
    if (torusKey.metadata.typeOfUser === "v1") {
      return torusKey.finalKeyData.privKey || torusKey.oAuthKeyData.privKey;
    }
    return torusKey.oAuthKeyData.privKey;
  }
  async retrieveShares(endpoints, indexes, verifier, verifierParams, idToken) {
    let extraParams = arguments.length > 5 && arguments[5] !== undefined ? arguments[5] : {};
    if (this.isLegacyNetwork) return this.legacyRetrieveShares(endpoints, indexes, verifier, verifierParams, idToken, extraParams);
    return retrieveOrImportShare({
      legacyMetadataHost: this.legacyMetadataHost,
      serverTimeOffset: this.serverTimeOffset,
      enableOneKey: this.enableOneKey,
      ecCurve: this.ec,
      allowHost: this.allowHost,
      network: this.network,
      clientId: this.clientId,
      endpoints,
      verifier,
      verifierParams,
      idToken,
      importedShares: [],
      extraParams
    });
  }
  async getPublicAddress(endpoints, torusNodePubs, _ref2) {
    let {
      verifier,
      verifierId,
      extendedVerifierId
    } = _ref2;
    if (this.isLegacyNetwork) return this.getLegacyPublicAddress(endpoints, torusNodePubs, {
      verifier,
      verifierId
    }, this.enableOneKey);
    return this.getNewPublicAddress(endpoints, {
      verifier,
      verifierId,
      extendedVerifierId
    }, this.enableOneKey);
  }
  async importPrivateKey(endpoints, nodeIndexes, nodePubkeys, verifier, verifierParams, idToken, newPrivateKey) {
    let extraParams = arguments.length > 7 && arguments[7] !== undefined ? arguments[7] : {};
    if (this.isLegacyNetwork) throw new Error("This function is not supported on legacy networks");
    if (endpoints.length !== nodeIndexes.length) {
      throw new Error(`length of endpoints array must be same as length of nodeIndexes array`);
    }
    const threshold = ~~(endpoints.length / 2) + 1;
    const degree = threshold - 1;
    const nodeIndexesBn = [];
    const key = this.ec.keyFromPrivate(newPrivateKey.padStart(64, "0"), "hex");
    for (const nodeIndex of nodeIndexes) {
      nodeIndexesBn.push(new BN(nodeIndex));
    }
    const privKeyBn = key.getPrivate();
    const randomNonce = new BN(generatePrivate());
    const oAuthKey = privKeyBn.sub(randomNonce).umod(this.ec.curve.n);
    const oAuthPubKey = this.ec.keyFromPrivate(oAuthKey.toString("hex").padStart(64, "0")).getPublic();
    const poly = generateRandomPolynomial(this.ec, degree, oAuthKey);
    const shares = poly.generateShares(nodeIndexesBn);
    const nonceParams = this.generateNonceMetadataParams("getOrSetNonce", oAuthKey, randomNonce);
    const nonceData = Buffer.from(stringify(nonceParams.set_data), "utf8").toString("base64");
    const sharesData = [];
    const encPromises = [];
    for (let i = 0; i < nodeIndexesBn.length; i++) {
      const shareJson = shares[nodeIndexesBn[i].toString("hex", 64)].toJSON();
      if (!nodePubkeys[i]) {
        throw new Error(`Missing node pub key for node index: ${nodeIndexesBn[i].toString("hex", 64)}`);
      }
      const nodePubKey = this.ec.keyFromPublic({
        x: nodePubkeys[i].X,
        y: nodePubkeys[i].Y
      });
      encPromises.push(encrypt(Buffer.from(nodePubKey.getPublic().encodeCompressed("hex"), "hex"), Buffer.from(shareJson.share, "hex")));
    }
    const encShares = await Promise.all(encPromises);
    for (let i = 0; i < nodeIndexesBn.length; i++) {
      const shareJson = shares[nodeIndexesBn[i].toString("hex", 64)].toJSON();
      const encParams = encShares[i];
      const encParamsMetadata = encParamsBufToHex(encParams);
      const shareData = {
        pub_key_x: oAuthPubKey.getX().toString("hex", 64),
        pub_key_y: oAuthPubKey.getY().toString("hex", 64),
        encrypted_share: encParamsMetadata.ciphertext,
        encrypted_share_metadata: encParamsMetadata,
        node_index: Number.parseInt(shareJson.shareIndex, 16),
        key_type: "secp256k1",
        nonce_data: nonceData,
        nonce_signature: nonceParams.signature
      };
      sharesData.push(shareData);
    }
    return retrieveOrImportShare({
      legacyMetadataHost: this.legacyMetadataHost,
      serverTimeOffset: this.serverTimeOffset,
      enableOneKey: this.enableOneKey,
      ecCurve: this.ec,
      allowHost: this.allowHost,
      network: this.network,
      clientId: this.clientId,
      endpoints,
      verifier,
      verifierParams,
      idToken,
      importedShares: sharesData,
      extraParams
    });
  }

  /**
   * Note: use this function only for openlogin tkey account lookups.
   * this is a legacy function, use getPublicAddress instead for new networks
   */
  async getUserTypeAndAddress(endpoints, torusNodePubs, _ref3) {
    let {
      verifier,
      verifierId,
      extendedVerifierId
    } = _ref3;
    if (!this.isLegacyNetwork) return this.getNewPublicAddress(endpoints, {
      verifier,
      verifierId,
      extendedVerifierId
    }, true);
    return this.getLegacyPublicAddress(endpoints, torusNodePubs, {
      verifier,
      verifierId
    }, true);
  }
  async legacyRetrieveShares(endpoints, indexes, verifier, verifierParams, idToken) {
    let extraParams = arguments.length > 5 && arguments[5] !== undefined ? arguments[5] : {};
    const promiseArr = [];
    await get(this.allowHost, {
      headers: {
        verifier,
        verifierId: verifierParams.verifier_id,
        network: this.network,
        clientId: this.clientId,
        enableGating: "true"
      }
    }, {
      useAPIKey: true
    });
    /*
      CommitmentRequestParams struct {
        MessagePrefix      string `json:"messageprefix"`
        TokenCommitment    string `json:"tokencommitment"`
        TempPubX           string `json:"temppubx"`
        TempPubY           string `json:"temppuby"`
        VerifierIdentifier string `json:"verifieridentifier"`
      } 
      */

    // generate temporary private and public key that is used to secure receive shares
    const tmpKey = generatePrivate();
    const pubKey = getPublic(tmpKey).toString("hex");
    const pubKeyX = pubKey.slice(2, 66);
    const pubKeyY = pubKey.slice(66);
    const tokenCommitment = keccak256(Buffer.from(idToken, "utf8"));

    // make commitment requests to endpoints
    for (let i = 0; i < endpoints.length; i += 1) {
      const p = post(endpoints[i], generateJsonRPCObject("CommitmentRequest", {
        messageprefix: "mug00",
        tokencommitment: tokenCommitment.slice(2),
        temppubx: pubKeyX,
        temppuby: pubKeyY,
        verifieridentifier: verifier
      })).catch(err => {
        log.error("commitment", err);
      });
      promiseArr.push(p);
    }
    /*
      ShareRequestParams struct {
        Item []bijson.RawMessage `json:"item"`
      }
      ShareRequestItem struct {
        IDToken            string          `json:"idtoken"`
        NodeSignatures     []NodeSignature `json:"nodesignatures"`
        VerifierIdentifier string          `json:"verifieridentifier"`
      }
      NodeSignature struct {
        Signature   string
        Data        string
        NodePubKeyX string
        NodePubKeyY string
      }
      CommitmentRequestResult struct {
        Signature string `json:"signature"`
        Data      string `json:"data"`
        NodePubX  string `json:"nodepubx"`
        NodePubY  string `json:"nodepuby"`
      }
      */
    // send share request once k + t number of commitment requests have completed
    return Some(promiseArr, resultArr => {
      const completedRequests = resultArr.filter(x => {
        if (!x || typeof x !== "object") {
          return false;
        }
        if (x.error) {
          return false;
        }
        return true;
      });
      if (completedRequests.length >= ~~(endpoints.length / 4) * 3 + 1) {
        return Promise.resolve(resultArr);
      }
      return Promise.reject(new Error(`invalid ${JSON.stringify(resultArr)}`));
    }).then(responses => {
      const promiseArrRequest = [];
      const nodeSigs = [];
      for (let i = 0; i < responses.length; i += 1) {
        if (responses[i]) nodeSigs.push(responses[i].result);
      }
      for (let i = 0; i < endpoints.length; i += 1) {
        const p = post(endpoints[i], generateJsonRPCObject("ShareRequest", {
          encrypted: "yes",
          item: [_objectSpread(_objectSpread({}, verifierParams), {}, {
            idtoken: idToken,
            nodesignatures: nodeSigs,
            verifieridentifier: verifier
          }, extraParams)]
        })).catch(err => log.error("share req", err));
        promiseArrRequest.push(p);
      }
      return Some(promiseArrRequest, async (shareResponses, sharedState) => {
        /*
            ShareRequestResult struct {
              Keys []KeyAssignment
            }
                    / KeyAssignmentPublic -
            type KeyAssignmentPublic struct {
              Index     big.Int
              PublicKey common.Point
              Threshold int
              Verifiers map[string][]string // Verifier => VerifierID
            }
             // KeyAssignment -
            type KeyAssignment struct {
              KeyAssignmentPublic
              Share big.Int // Or Si
            }
          */
        // check if threshold number of nodes have returned the same user public key
        const completedRequests = shareResponses.filter(x => x);
        const thresholdPublicKey = thresholdSame(shareResponses.map(x => x && x.result && x.result.keys[0].PublicKey), ~~(endpoints.length / 2) + 1);
        // optimistically run lagrange interpolation once threshold number of shares have been received
        // this is matched against the user public key to ensure that shares are consistent
        if (completedRequests.length >= ~~(endpoints.length / 2) + 1 && thresholdPublicKey) {
          const sharePromises = [];
          const nodeIndexes = [];
          for (let i = 0; i < shareResponses.length; i += 1) {
            var _currentShareResponse;
            const currentShareResponse = shareResponses[i];
            if ((currentShareResponse === null || currentShareResponse === void 0 || (_currentShareResponse = currentShareResponse.result) === null || _currentShareResponse === void 0 || (_currentShareResponse = _currentShareResponse.keys) === null || _currentShareResponse === void 0 ? void 0 : _currentShareResponse.length) > 0) {
              currentShareResponse.result.keys.sort((a, b) => new BN(a.Index, 16).cmp(new BN(b.Index, 16)));
              const firstKey = currentShareResponse.result.keys[0];
              if (firstKey.Metadata) {
                const metadata = {
                  ephemPublicKey: Buffer.from(firstKey.Metadata.ephemPublicKey, "hex"),
                  iv: Buffer.from(firstKey.Metadata.iv, "hex"),
                  mac: Buffer.from(firstKey.Metadata.mac, "hex")
                  // mode: Buffer.from(firstKey.Metadata.mode, "hex"),
                };

                sharePromises.push(decrypt(tmpKey, _objectSpread(_objectSpread({}, metadata), {}, {
                  ciphertext: Buffer.from(Buffer.from(firstKey.Share, "base64").toString("binary").padStart(64, "0"), "hex")
                })).catch(err => log.debug("share decryption", err)));
              } else {
                sharePromises.push(Promise.resolve(Buffer.from(firstKey.Share.padStart(64, "0"), "hex")));
              }
            } else {
              sharePromises.push(Promise.resolve(undefined));
            }
            nodeIndexes.push(new BN(indexes[i], 16));
          }
          const sharesResolved = await Promise.all(sharePromises);
          if (sharedState.resolved) return undefined;
          const decryptedShares = sharesResolved.reduce((acc, curr, index) => {
            if (curr) acc.push({
              index: nodeIndexes[index],
              value: new BN(curr)
            });
            return acc;
          }, []);
          // run lagrange interpolation on all subsets, faster in the optimistic scenario than berlekamp-welch due to early exit
          const allCombis = kCombinations(decryptedShares.length, ~~(endpoints.length / 2) + 1);
          let privateKey = null;
          for (let j = 0; j < allCombis.length; j += 1) {
            const currentCombi = allCombis[j];
            const currentCombiShares = decryptedShares.filter((_, index) => currentCombi.includes(index));
            const shares = currentCombiShares.map(x => x.value);
            const indices = currentCombiShares.map(x => x.index);
            const derivedPrivateKey = lagrangeInterpolation(this.ec, shares, indices);
            if (!derivedPrivateKey) continue;
            const decryptedPubKey = getPublic(Buffer.from(derivedPrivateKey.toString(16, 64), "hex")).toString("hex");
            const decryptedPubKeyX = decryptedPubKey.slice(2, 66);
            const decryptedPubKeyY = decryptedPubKey.slice(66);
            if (new BN(decryptedPubKeyX, 16).cmp(new BN(thresholdPublicKey.X, 16)) === 0 && new BN(decryptedPubKeyY, 16).cmp(new BN(thresholdPublicKey.Y, 16)) === 0) {
              privateKey = derivedPrivateKey;
              break;
            }
          }
          if (privateKey === undefined || privateKey === null) {
            throw new Error("could not derive private key");
          }
          return privateKey;
        }
        throw new Error("invalid");
      });
    }).then(async returnedKey => {
      const oAuthKey = returnedKey;
      if (!oAuthKey) throw new Error("Invalid private key returned");
      const oAuthPubKey = getPublic(Buffer.from(oAuthKey.toString(16, 64), "hex")).toString("hex");
      const oAuthKeyX = oAuthPubKey.slice(2, 66);
      const oAuthKeyY = oAuthPubKey.slice(66);
      let metadataNonce;
      let finalPubKey;
      let typeOfUser = "v1";
      let pubKeyNonceResult;
      if (this.enableOneKey) {
        const nonceResult = await getNonce(this.legacyMetadataHost, this.ec, this.serverTimeOffset, oAuthKeyX, oAuthKeyY, oAuthKey);
        metadataNonce = new BN(nonceResult.nonce || "0", 16);
        typeOfUser = nonceResult.typeOfUser;
        if (typeOfUser === "v2") {
          finalPubKey = this.ec.keyFromPublic({
            x: oAuthKeyX,
            y: oAuthKeyY
          }).getPublic().add(this.ec.keyFromPublic({
            x: nonceResult.pubNonce.x,
            y: nonceResult.pubNonce.y
          }).getPublic());
          pubKeyNonceResult = {
            X: nonceResult.pubNonce.x,
            Y: nonceResult.pubNonce.y
          };
        } else {
          // for imported keys in legacy networks
          metadataNonce = await getMetadata(this.legacyMetadataHost, {
            pub_key_X: oAuthKeyX,
            pub_key_Y: oAuthKeyY
          });
          const privateKeyWithNonce = oAuthKey.add(metadataNonce).umod(this.ec.curve.n);
          finalPubKey = this.ec.keyFromPrivate(privateKeyWithNonce.toString("hex"), "hex").getPublic();
        }
      } else {
        // for imported keys in legacy networks
        metadataNonce = await getMetadata(this.legacyMetadataHost, {
          pub_key_X: oAuthKeyX,
          pub_key_Y: oAuthKeyY
        });
        const privateKeyWithNonce = oAuthKey.add(metadataNonce).umod(this.ec.curve.n);
        finalPubKey = this.ec.keyFromPrivate(privateKeyWithNonce.toString("hex"), "hex").getPublic();
      }
      const oAuthKeyAddress = generateAddressFromPrivKey(this.ec, oAuthKey);
      let finalPrivKey = ""; // it is empty for v2 user upgraded to 2/n
      if (typeOfUser === "v1" || typeOfUser === "v2" && metadataNonce.gt(new BN(0))) {
        const privateKeyWithNonce = oAuthKey.add(metadataNonce).umod(this.ec.curve.n);
        finalPrivKey = privateKeyWithNonce.toString("hex", 64).padStart(64, "0");
      }
      let isUpgraded = false;
      if (typeOfUser === "v1") {
        isUpgraded = null;
      } else if (typeOfUser === "v2") {
        isUpgraded = metadataNonce.eq(new BN("0"));
      }

      // deriving address from pub key coz pubkey is always available
      // but finalPrivKey won't be available for  v2 user upgraded to 2/n
      let finalEvmAddress = "";
      if (finalPubKey) {
        finalEvmAddress = generateAddressFromPubKey(this.ec, finalPubKey.getX(), finalPubKey.getY());
        log.debug("> torus.js/retrieveShares", {
          finalEvmAddress
        });
      } else {
        throw new Error("Invalid public key, this might be a bug, please report this to web3auth team");
      }
      return {
        finalKeyData: {
          evmAddress: finalEvmAddress,
          X: finalPubKey ? finalPubKey.getX().toString(16, 64) : "",
          // this is final pub x user before and after updating to 2/n
          Y: finalPubKey ? finalPubKey.getY().toString(16, 64) : "",
          // this is final pub y user before and after updating to 2/n
          privKey: finalPrivKey
        },
        oAuthKeyData: {
          evmAddress: oAuthKeyAddress,
          X: oAuthKeyX,
          Y: oAuthKeyY,
          privKey: oAuthKey.toString("hex", 64).padStart(64, "0")
        },
        sessionData: {
          sessionTokenData: [],
          sessionAuthKey: ""
        },
        metadata: {
          pubNonce: pubKeyNonceResult,
          nonce: metadataNonce,
          typeOfUser: typeOfUser,
          upgraded: isUpgraded
        },
        nodesData: {
          nodeIndexes: []
        }
      };
    });
  }
  async getLegacyPublicAddress(endpoints, torusNodePubs, _ref4, enableOneKey) {
    let {
      verifier,
      verifierId
    } = _ref4;
    log.debug("> torus.js/getPublicAddress", {
      endpoints,
      torusNodePubs,
      verifier,
      verifierId
    });
    let finalKeyResult;
    let isNewKey = false;
    const {
      keyResult,
      errorResult
    } = (await legacyKeyLookup(endpoints, verifier, verifierId)) || {};
    if (errorResult && JSON.stringify(errorResult).includes("Verifier not supported")) {
      // change error msg
      throw new Error(`Verifier not supported. Check if you: \n
      1. Are on the right network (Torus testnet/mainnet) \n
      2. Have setup a verifier on dashboard.web3auth.io?`);
    } else if (errorResult && JSON.stringify(errorResult).includes("Verifier + VerifierID has not yet been assigned")) {
      await legacyKeyAssign({
        endpoints,
        torusNodePubs,
        lastPoint: undefined,
        firstPoint: undefined,
        verifier,
        verifierId,
        signerHost: this.signerHost,
        network: this.network,
        clientId: this.clientId
      });
      const assignResult = await legacyWaitKeyLookup(endpoints, verifier, verifierId, 1000);
      finalKeyResult = assignResult === null || assignResult === void 0 ? void 0 : assignResult.keyResult;
      isNewKey = true;
    } else if (keyResult) {
      finalKeyResult = keyResult;
    } else {
      throw new Error(`node results do not match at first lookup ${JSON.stringify(keyResult || {})}, ${JSON.stringify(errorResult || {})}`);
    }
    log.debug("> torus.js/getPublicAddress", {
      finalKeyResult,
      isNewKey
    });
    if (finalKeyResult) {
      return this.formatLegacyPublicKeyData({
        finalKeyResult,
        isNewKey,
        enableOneKey
      });
    }
    throw new Error(`node results do not match at final lookup ${JSON.stringify(keyResult || {})}, ${JSON.stringify(errorResult || {})}`);
  }
  generateNonceMetadataParams(operation, privateKey, nonce) {
    const key = this.ec.keyFromPrivate(privateKey.toString("hex", 64));
    const setData = {
      operation,
      timestamp: new BN(~~(this.serverTimeOffset + Date.now() / 1000)).toString(16)
    };
    if (nonce) {
      setData.data = nonce.toString("hex", 64);
    }
    const sig = key.sign(keccak256(Buffer.from(stringify(setData), "utf8")).slice(2));
    return {
      pub_key_X: key.getPublic().getX().toString("hex", 64),
      pub_key_Y: key.getPublic().getY().toString("hex", 64),
      set_data: setData,
      signature: Buffer.from(sig.r.toString(16, 64) + sig.s.toString(16, 64) + new BN("").toString(16, 2), "hex").toString("base64")
    };
  }
  async getNewPublicAddress(endpoints, _ref5, enableOneKey) {
    let {
      verifier,
      verifierId,
      extendedVerifierId
    } = _ref5;
    log.debug("> torus.js/getPublicAddress", {
      endpoints,
      verifier,
      verifierId
    });
    const keyAssignResult = await GetPubKeyOrKeyAssign({
      endpoints,
      network: this.network,
      verifier,
      verifierId,
      extendedVerifierId
    });
    const {
      errorResult,
      keyResult,
      nodeIndexes = []
    } = keyAssignResult;
    const {
      nonceResult
    } = keyAssignResult;
    if (errorResult && JSON.stringify(errorResult).toLowerCase().includes("verifier not supported")) {
      // change error msg
      throw new Error(`Verifier not supported. Check if you: \n
      1. Are on the right network (Torus testnet/mainnet) \n
      2. Have setup a verifier on dashboard.web3auth.io?`);
    }
    if (errorResult) {
      throw new Error(`node results do not match at first lookup ${JSON.stringify(keyResult || {})}, ${JSON.stringify(errorResult || {})}`);
    }
    log.debug("> torus.js/getPublicAddress", {
      keyResult
    });
    if (!(keyResult !== null && keyResult !== void 0 && keyResult.keys)) {
      throw new Error(`node results do not match at final lookup ${JSON.stringify(keyResult || {})}, ${JSON.stringify(errorResult || {})}`);
    }

    // no need of nonce for extendedVerifierId (tss verifier id)
    if (!nonceResult && !extendedVerifierId && !LEGACY_NETWORKS_ROUTE_MAP[this.network]) {
      throw new GetOrSetNonceError("metadata nonce is missing in share response");
    }
    const {
      pub_key_X: X,
      pub_key_Y: Y
    } = keyResult.keys[0];
    let pubNonce;
    const nonce = new BN((nonceResult === null || nonceResult === void 0 ? void 0 : nonceResult.nonce) || "0", 16);
    let oAuthPubKey;
    let finalPubKey;
    if (extendedVerifierId) {
      // for tss key no need to add pub nonce
      finalPubKey = this.ec.keyFromPublic({
        x: X,
        y: Y
      }).getPublic();
      oAuthPubKey = finalPubKey;
    } else if (LEGACY_NETWORKS_ROUTE_MAP[this.network]) {
      return this.formatLegacyPublicKeyData({
        isNewKey: keyResult.is_new_key,
        enableOneKey,
        finalKeyResult: {
          keys: keyResult.keys
        }
      });
    } else {
      const v2NonceResult = nonceResult;
      oAuthPubKey = this.ec.keyFromPublic({
        x: X,
        y: Y
      }).getPublic();
      finalPubKey = this.ec.keyFromPublic({
        x: X,
        y: Y
      }).getPublic().add(this.ec.keyFromPublic({
        x: v2NonceResult.pubNonce.x,
        y: v2NonceResult.pubNonce.y
      }).getPublic());
      pubNonce = {
        X: v2NonceResult.pubNonce.x,
        Y: v2NonceResult.pubNonce.y
      };
    }
    if (!oAuthPubKey) {
      throw new Error("Unable to derive oAuthPubKey");
    }
    const oAuthX = oAuthPubKey.getX().toString(16, 64);
    const oAuthY = oAuthPubKey.getY().toString(16, 64);
    const oAuthAddress = generateAddressFromPubKey(this.ec, oAuthPubKey.getX(), oAuthPubKey.getY());
    log.debug("> torus.js/getPublicAddress, oAuthKeyData", {
      X: oAuthX,
      Y: oAuthY,
      oAuthAddress,
      nonce: nonce === null || nonce === void 0 ? void 0 : nonce.toString(16),
      pubNonce
    });
    if (!finalPubKey) {
      throw new Error("Unable to derive finalPubKey");
    }
    const finalX = finalPubKey ? finalPubKey.getX().toString(16, 64) : "";
    const finalY = finalPubKey ? finalPubKey.getY().toString(16, 64) : "";
    const finalAddress = finalPubKey ? generateAddressFromPubKey(this.ec, finalPubKey.getX(), finalPubKey.getY()) : "";
    return {
      oAuthKeyData: {
        evmAddress: oAuthAddress,
        X: oAuthX,
        Y: oAuthY
      },
      finalKeyData: {
        evmAddress: finalAddress,
        X: finalX,
        Y: finalY
      },
      metadata: {
        pubNonce,
        nonce,
        upgraded: (nonceResult === null || nonceResult === void 0 ? void 0 : nonceResult.upgraded) || false,
        typeOfUser: "v2"
      },
      nodesData: {
        nodeIndexes
      }
    };
  }
  async formatLegacyPublicKeyData(params) {
    var _nonce, _nonceResult;
    const {
      finalKeyResult,
      enableOneKey,
      isNewKey
    } = params;
    const {
      pub_key_X: X,
      pub_key_Y: Y
    } = finalKeyResult.keys[0];
    let nonceResult;
    let nonce;
    let finalPubKey;
    let typeOfUser;
    let pubNonce;
    const oAuthPubKey = this.ec.keyFromPublic({
      x: X,
      y: Y
    }).getPublic();
    if (enableOneKey) {
      try {
        nonceResult = await getOrSetNonce(this.legacyMetadataHost, this.ec, this.serverTimeOffset, X, Y, undefined, !isNewKey);
        nonce = new BN(nonceResult.nonce || "0", 16);
        typeOfUser = nonceResult.typeOfUser;
      } catch {
        throw new GetOrSetNonceError();
      }
      if (nonceResult.typeOfUser === "v1") {
        nonce = await getMetadata(this.legacyMetadataHost, {
          pub_key_X: X,
          pub_key_Y: Y
        });
        finalPubKey = this.ec.keyFromPublic({
          x: X,
          y: Y
        }).getPublic().add(this.ec.keyFromPrivate(nonce.toString(16, 64), "hex").getPublic());
      } else if (nonceResult.typeOfUser === "v2") {
        finalPubKey = this.ec.keyFromPublic({
          x: X,
          y: Y
        }).getPublic().add(this.ec.keyFromPublic({
          x: nonceResult.pubNonce.x,
          y: nonceResult.pubNonce.y
        }).getPublic());
        pubNonce = {
          X: nonceResult.pubNonce.x,
          Y: nonceResult.pubNonce.y
        };
      } else {
        throw new Error("getOrSetNonce should always return typeOfUser.");
      }
    } else {
      typeOfUser = "v1";
      nonce = await getMetadata(this.legacyMetadataHost, {
        pub_key_X: X,
        pub_key_Y: Y
      });
      finalPubKey = this.ec.keyFromPublic({
        x: X,
        y: Y
      }).getPublic().add(this.ec.keyFromPrivate(nonce.toString(16, 64), "hex").getPublic());
    }
    if (!oAuthPubKey) {
      throw new Error("Unable to derive oAuthPubKey");
    }
    const oAuthX = oAuthPubKey.getX().toString(16, 64);
    const oAuthY = oAuthPubKey.getY().toString(16, 64);
    const oAuthAddress = generateAddressFromPubKey(this.ec, oAuthPubKey.getX(), oAuthPubKey.getY());
    log.debug("> torus.js/getPublicAddress, oAuthKeyData", {
      X: oAuthX,
      Y: oAuthY,
      oAuthAddress,
      nonce: (_nonce = nonce) === null || _nonce === void 0 ? void 0 : _nonce.toString(16),
      pubNonce
    });
    if (typeOfUser === "v2" && !finalPubKey) {
      throw new Error("Unable to derive finalPubKey");
    }
    const finalX = finalPubKey ? finalPubKey.getX().toString(16, 64) : "";
    const finalY = finalPubKey ? finalPubKey.getY().toString(16, 64) : "";
    const finalAddress = finalPubKey ? generateAddressFromPubKey(this.ec, finalPubKey.getX(), finalPubKey.getY()) : "";
    return {
      oAuthKeyData: {
        evmAddress: oAuthAddress,
        X: oAuthX,
        Y: oAuthY
      },
      finalKeyData: {
        evmAddress: finalAddress,
        X: finalX,
        Y: finalY
      },
      metadata: {
        pubNonce,
        nonce,
        upgraded: ((_nonceResult = nonceResult) === null || _nonceResult === void 0 ? void 0 : _nonceResult.upgraded) || false,
        typeOfUser
      },
      nodesData: {
        nodeIndexes: []
      }
    };
  }
}

export { GetOrSetNonceError, GetPubKeyOrKeyAssign, JRPC_METHODS, Point, Polynomial, Share, convertMetadataToNonce, decryptNodeData, Torus as default, encParamsBufToHex, encParamsHexToBuf, generateAddressFromPrivKey, generateAddressFromPubKey, generateMetadataParams, generateRandomPolynomial, getMetadata, getNonce, getOrSetNonce, getPostboxKeyFrom1OutOf1, kCombinations, keccak256, lagrangeInterpolatePolynomial, lagrangeInterpolation, legacyKeyAssign, legacyKeyLookup, legacyWaitKeyLookup, normalizeKeysResult, retrieveOrImportShare, stripHexPrefix, thresholdSame, toChecksumAddress };
//# sourceMappingURL=torusUtils.esm.js.map
